{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a002f6e2",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import mne\n",
    "from mne import EpochsArray, create_info\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80048b08",
   "metadata": {},
   "source": [
    "To save the figures that are not MNEQtBrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save all the Figures that are NOT MNEQtBrowser\n",
    "def save_figure(fig, filename, folder=\"C:\\\\Users\\\\indira.lavocat\\\\MOVIDOC\\\\tictrack_eeg_analysis\\\\Figures\"):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    fig_path = os.path.join(folder, filename)\n",
    "    fig.savefig(fig_path)\n",
    "    print(f\"Figure saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79a719",
   "metadata": {},
   "source": [
    "# A. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440670b",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the .vhdr file\n",
    "FolderPath = \"C:\\\\Users\\\\indira.lavocat\\\\MOVIDOC\\\\EEG\\\\Sujets\\\\IndiraLAVOCAT\" # need to adapt the last folder to suit the subject\n",
    "\n",
    "# Looking for the .vhdr file in the folder\n",
    "for file in os.listdir(FolderPath):\n",
    "    if file.endswith(\".vhdr\"):\n",
    "        FilePath = os.path.join(FolderPath, file)\n",
    "        break\n",
    "\n",
    "print(FilePath)\n",
    "\n",
    "\n",
    "# vhdr_file = \"C:\\\\Users\\\\indira.lavocat\\\\MOVIDOC\\\\EEG\\\\Indira Test\\\\MOVIDOCTicTrack000005.vhdr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_brainvision(FilePath, preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf83d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a1e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.ch_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6079d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.info['description']) # gives a note about the channels when there is one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bf00e",
   "metadata": {},
   "source": [
    "## 2. Extract the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f311ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an events dicionnary\n",
    "events, event_id = mne.events_from_annotations(raw)\n",
    "print(\"Events list (stimulus) :\")\n",
    "print(event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb00598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the tab of events\n",
    "print(\"Events (sample, previous_id, event_id) :\")\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamps into seconds\n",
    "events_no_zero = events[events[:, 0] != 0]  # <-- filters the events at 0.000 s\n",
    "events_times_sec = events_no_zero[:, 0] / raw.info['sfreq'] # converts the timestamps into seconds\n",
    "\n",
    "for time, eid in zip(events_times_sec, events_no_zero[:, 2]): # links each time in seconds to its event ID\n",
    "    print(f\"Stimulus {eid} à {time:.3f} s\") # formats the number with 3 decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647defd",
   "metadata": {},
   "source": [
    "Alternative to display the stimulus name (and not its ID) with its timestamp in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df49ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {v: k for k, v in event_id.items()}\n",
    "\n",
    "for time, eid in zip(events_times_sec, events_no_zero[:, 2]):\n",
    "    name = id_to_name.get(eid, f\"ID {eid}\")\n",
    "    print(f\"{name} à {time:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2024ecb",
   "metadata": {},
   "source": [
    "### NOT TO DO : Alternative whith a pandas DataFrame output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {v: k for k, v in event_id.items()}\n",
    "\n",
    "events_data = [] # création of dictionnaries list (one per event)\n",
    "for time, eid in zip(events_times_sec, events[:, 2]):\n",
    "    name = id_to_name.get(eid, f\"ID {eid}\")\n",
    "    events_data.append({\n",
    "        \"Nom de l'événement\": name,\n",
    "        \"ID\": eid,\n",
    "        \"Temps (s)\": round(time, 3)\n",
    "    })\n",
    "\n",
    "df_events = pd.DataFrame(events_data) # conversion into a DataFrame\n",
    "\n",
    "print(df_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8484e70",
   "metadata": {},
   "source": [
    "# B. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7921c8",
   "metadata": {},
   "source": [
    "## 1. Define the montage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1522c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.set_montage(\"standard_1020\") # to adapt according to the montage used during the exepriments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sensors_Montage_Figure_1 = raw.plot_sensors(show_names=True)\n",
    "save_figure(Sensors_Montage_Figure_1, \"Figure1_SensorsMontage.png\")\n",
    "# fig.savefig(\"C:\\\\Users\\\\indira.lavocat\\\\MOVIDOC\\\\tictrack_eeg_analysis\\\\Figures\\\\Figure1_SensorsMontage.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb9835",
   "metadata": {},
   "source": [
    "## 2. Filter the data with a band-pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d18eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the high and low frequencies\n",
    "HFreq = 45\n",
    "LFreq = 0.5\n",
    "raw_HighLowPassed = raw.filter(l_freq = LFreq, h_freq = HFreq)\n",
    "\n",
    "# for ERPs, [1-30] Hz band-pass filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the highpassed signal\n",
    "Signal_HighLowPassed_Figure_2 = raw_HighLowPassed.plot(title = \"High- and Low- passed Signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66bed48",
   "metadata": {},
   "source": [
    "## 3. Filter the data with a Notch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the notch filter\n",
    "if HFreq < 50:\n",
    "    raw_Notched = raw_HighLowPassed\n",
    "else:\n",
    "    raw_Notched = raw_HighLowPassed.notch_filter(freqs = [50], picks = \"data\", method = \"spectrum_fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d31d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the notched signal\n",
    "Signal_Notched_Figure_3 = raw_Notched.plot(title = \"Notched Signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d2712",
   "metadata": {},
   "source": [
    "## 4. Identify the bad channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abf04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11100417",
   "metadata": {},
   "source": [
    "## 5. Re-reference the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST method : advanced EEG re-referencemen\n",
    "print(\"Application du référentiel REST...\")\n",
    "\n",
    "# Create a spherical model of the head based on the file information \n",
    "Sphere = mne.make_sphere_model('auto', 'auto', raw_Notched.info)\n",
    "\n",
    "# Define the volume source space\n",
    "Source = mne.setup_volume_source_space(sphere=Sphere, exclude=30.0, pos=5.0, mri=None, verbose=False)\n",
    "\n",
    "# Calculate the forward model solution\n",
    "Forward = mne.make_forward_solution(raw_Notched.info, trans=None, src=Source, bem=Sphere, verbose=False)\n",
    "\n",
    "# Apply the REST reference\n",
    "raw_REST = raw_Notched.copy().set_eeg_reference('REST', forward=Forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f5ae8",
   "metadata": {},
   "source": [
    "Optionnal : visualisation after REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the signal (after REST)\n",
    "Signal_REST_Figure_4 = raw_REST.plot(title=\"Signal après référence REST\")\n",
    "save_figure(Signal_REST_Figure_4, \"Figure4_Signal_REST.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5ba02",
   "metadata": {},
   "source": [
    "## 7. Recalage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the file time\n",
    "if events_times_sec[0] == 0 and len(events_times_sec) > 1: # check if the 1st stimulus is at 0 s. If so, use the 2nd stimulus\n",
    "    first_stimulus_time = events_times_sec[1]\n",
    "    print(f\"First stimulus is at 0. Using second stimulus at {first_stimulus_time:.3f} s\")\n",
    "else:\n",
    "    first_stimulus_time = events_times_sec[0]\n",
    "    print(f\"First stimulus at {first_stimulus_time:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the signal to start at this point\n",
    "raw_cropped = raw_REST.copy().crop(tmin=first_stimulus_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the annotations by shifting all annotations by - first_stimulus_time\n",
    "if raw.annotations is not None:\n",
    "    raw_annotation_times = raw.annotations.onset - first_stimulus_time\n",
    "    raw_cropped.set_annotations(\n",
    "        mne.Annotations(\n",
    "            onset=raw_annotation_times, # onset = raw_annotation_times with the new reset times\n",
    "            duration=raw.annotations.duration,\n",
    "            description=raw.annotations.description\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba01f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot truncated and recalculated data\n",
    "Readjusted_Signal_Figure_5 = raw_cropped.plot(title=\"Readjusted signal (from the 1st stimulus not at 0 s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df49c8a",
   "metadata": {},
   "source": [
    "# C. Epoching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de642e0",
   "metadata": {},
   "source": [
    "## 1. Phase 1 (P1)\n",
    "### Get the \"press a key\" baseline from the P1 phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0d9b0",
   "metadata": {},
   "source": [
    "Get the stimuli in P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "begin_P1_stimulus = \"Stimulus/S  3\" # sent at the beginning of the P1 task\n",
    "end_P1_stimulus = \"Stimulus/S  4\" # sent at the ending of the P1 task \n",
    "\n",
    "# Create a list with annotations and their times\n",
    "list_annotations = list(zip(raw_cropped.annotations.onset, raw_cropped.annotations.description))\n",
    "\n",
    "# Find all the segments in the P1 phase\n",
    "P1_segments = []\n",
    "i = 0\n",
    "while i < len(list_annotations):\n",
    "    onset, desc = list_annotations[i]\n",
    "    if desc == begin_P1_stimulus:\n",
    "        # Search the next end event\n",
    "        for j in range(i + 1, len(list_annotations)):\n",
    "            next_onset, next_desc = list_annotations[j]\n",
    "            if next_desc == end_P1_stimulus:\n",
    "                P1_segments.append((onset, next_onset))\n",
    "                i = j  # continue after the stop\n",
    "                break\n",
    "    i += 1\n",
    "\n",
    "# Get all the stimuli present in the segments\n",
    "stimuli_in_P1 = []\n",
    "for start, end in P1_segments:\n",
    "    for onset, desc in list_annotations:\n",
    "        if start <= onset <= end:\n",
    "            # Do not take into account the beginning and ending stimuli\n",
    "            if desc not in (begin_P1_stimulus, end_P1_stimulus):\n",
    "                stimuli_in_P1.append((onset, desc))\n",
    "\n",
    "# Display & check the stimuli found\n",
    "print(f\"{len(stimuli_in_P1)} stimuli trouvés entre '{begin_P1_stimulus}' et '{end_P1_stimulus}' :\")\n",
    "for onset, desc in stimuli_in_P1:\n",
    "    print(f\"{desc} à {onset:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c55091",
   "metadata": {},
   "source": [
    "Get the the signal -1 second before & +1 second after each stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window around each stimulus\n",
    "tmin = -1.0 # 1 seconde before\n",
    "tmax = 1.0 # 1 seconde after\n",
    "\n",
    "# Get the sampling frequency\n",
    "sfreq = raw_cropped.info['sfreq']\n",
    "\n",
    "# Initiate a list to stock the extracted values (expected shape per segment : n_channels x n_times)\n",
    "P1_signal_segments = []\n",
    "\n",
    "for onset, desc in stimuli_in_P1:\n",
    "    start = onset + tmin\n",
    "    end = onset + tmax\n",
    "    # Check if the window is not outside the signal bounds\n",
    "    if start < 0 or end > raw_cropped.times[-1]:\n",
    "        print(f\"⚠️ Stimulus at {onset:.2f}s ignored (window [{start:.2f}, {end:.2f}] out of limits)\")\n",
    "        continue\n",
    "    # Get the segment\n",
    "    P1_segment = raw_cropped.copy().crop(tmin=start, tmax=end).get_data() # shape: (n_channels, n_times)\n",
    "    P1_signal_segments.append(P1_segment)\n",
    "\n",
    "# Convert into an array numpy : shape = (n_events, n_channels, n_times)\n",
    "P1_segments_array = np.array(P1_signal_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618376a0",
   "metadata": {},
   "source": [
    "Create epochs for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an info object to build an EpochsArray object\n",
    "info = create_info(\n",
    "    ch_names=raw_cropped.ch_names,\n",
    "    sfreq=sfreq,\n",
    "    ch_types=\"eeg\"  # to adapt according to the sensors nature\n",
    ")\n",
    "\n",
    "# Create the EpochsArray object from P1_segments_array\n",
    "epochs_P1 = EpochsArray(P1_segments_array, info) # expected shape : (n_epochs, n_channels, n_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a .fif file\n",
    "save_P1_path = \"C:\\\\Users\\\\indira.lavocat\\\\MOVIDOC\\\\tictrack_eeg_analysis\\\\.fif_files\\\\P1_epochs.fif\"\n",
    "epochs_P1.save(save_P1_path, overwrite=True)\n",
    "print(f\"✅ Segments sauvegardés dans : {save_P1_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0416aed",
   "metadata": {},
   "source": [
    "#### OPTIONNAL : Display the mean segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean over all the P1 events : shape = (n_channels, n_times)\n",
    "mean_segment = np.mean(P1_segments_array, axis=0)\n",
    "\n",
    "# Display the result shape\n",
    "print(f\"\\n✅ {len(P1_signal_segments)} valide segments used.\")\n",
    "print(f\"Shape of the mean segment : {mean_segment.shape} (n_channels, n_times)\")\n",
    "\n",
    "\n",
    "# Create the temporal axis for the mean segment\n",
    "n_times = mean_segment.shape[1]\n",
    "times = np.linspace(tmin, tmax, n_times)\n",
    "\n",
    "# Trace each channel in a seperated figure\n",
    "for ch_idx, ch_name in enumerate(raw_cropped.ch_names):\n",
    "    signal = mean_segment[ch_idx, :]\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(times, signal, label=f'Channel : {ch_name}')\n",
    "    plt.title(f\"Mean segment – Channel {ch_name}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude (µV)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e150f2e",
   "metadata": {},
   "source": [
    "#### NO TO DO : Get the mean of the signal values at the exact time of each stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a list to stock the extracted values\n",
    "signal_values_at_stimulus = []\n",
    "\n",
    "# Get the signal value for each stimulus\n",
    "for onset, desc in stimuli_in_P1:\n",
    "    sample_index = int(onset * raw_cropped.info['sfreq'])  # convertir le temps en index d'échantillon\n",
    "    signal_sample = raw_cropped[:, sample_index][0]  # shape = (n_channels,)\n",
    "    signal_values_at_stimulus.append(signal_sample)\n",
    "\n",
    "# Convert into an array (shape: n_events x n_channels)\n",
    "signal_values_array = np.array(signal_values_at_stimulus)  # shape = (n_events, n_channels)\n",
    "\n",
    "# Mean of all the events (shape: n_channels,)\n",
    "mean_signal_per_channel = np.mean(signal_values_array, axis=0)\n",
    "\n",
    "# Display the mean signal from the stimuli in P1 phase\n",
    "print(\"Mean value of the signal from the stimuli in P1 phase (per channel) :\")\n",
    "for ch_name, value in zip(raw_cropped.ch_names, mean_signal_per_channel):\n",
    "    print(f\"{ch_name}: {value:.3f} µV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98061cfa",
   "metadata": {},
   "source": [
    "## 2. Phase 2 (P2)\n",
    "### Get the \"eyes closed\" baseline from the P2a phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765dcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "begin_P2a_stimulus = \"Stimulus/S  5\" # sent at the beginning of the P2a task\n",
    "end_P2a_stimulus = \"Stimulus/S  6\" # sent at the ending of the P2a task\n",
    "\n",
    "# Search the stimuli times in the annotations\n",
    "onset_start = None\n",
    "onset_end = None\n",
    "\n",
    "for onset, desc in zip(raw_cropped.annotations.onset, raw_cropped.annotations.description):\n",
    "    if desc == begin_P2a_stimulus and onset_start is None:\n",
    "        onset_start = onset\n",
    "    elif desc == end_P2a_stimulus and onset_start is not None:\n",
    "        onset_end = onset\n",
    "        break # stopping the loop as soon as the pair of stimuli is found\n",
    "\n",
    "# Check the segment found\n",
    "if onset_start is not None and onset_end is not None:\n",
    "    print(f\"Segment detected : from {onset_start:.2f} s to {onset_end:.2f} s\")\n",
    "\n",
    "    # Get the signal segment\n",
    "    P2a_segment = raw_cropped.copy().crop(tmin=onset_start, tmax=onset_end)\n",
    "    data, times = P2a_segment.get_data(return_times=True) # data shape: (n_channels, n_times)\n",
    "\n",
    "    # Calculate the mean absolute value (over the time axis)\n",
    "    mean_abs_values = np.mean(np.abs(data), axis=1) # shape: (n_channels,)\n",
    "    \n",
    "    # Display the mean absolute value for each channel\n",
    "    for ch_name, mean_val in zip(raw_cropped.ch_names, mean_abs_values):\n",
    "        # print(f\"{ch_name} : mean absolute value = {mean_val:.3f} µV\")\n",
    "        print(f\"{ch_name} : mean absolute value = {mean_val:.8f} µV\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Stimuli not found in the annotations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58828453",
   "metadata": {},
   "source": [
    "### 8.1.c. Get the \"eyes open\" baseline from the P2b phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08861df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "begin_P2b_stimulus = \"Stimulus/S  7\" # sent at the beginning of the P2a task\n",
    "end_P2b_stimulus = \"Stimulus/S  8\" # sent at the ending of the P2a task\n",
    "\n",
    "# Search the stimuli times in the annotations\n",
    "onset_start = None\n",
    "onset_end = None\n",
    "\n",
    "for onset, desc in zip(raw_cropped.annotations.onset, raw_cropped.annotations.description):\n",
    "    if desc == begin_P2b_stimulus and onset_start is None:\n",
    "        onset_start = onset\n",
    "    elif desc == end_P2b_stimulus and onset_start is not None:\n",
    "        onset_end = onset\n",
    "        break # stopping the loop as soon as the pair of stimuli is found\n",
    "\n",
    "# Check the segment found\n",
    "if onset_start is not None and onset_end is not None:\n",
    "    print(f\"Segment detected : from {onset_start:.2f} s to {onset_end:.2f} s\")\n",
    "\n",
    "    # Get the signal segment\n",
    "    P2b_segment = raw_cropped.copy().crop(tmin=onset_start, tmax=onset_end)\n",
    "    data, times = P2b_segment.get_data(return_times=True) # data shape: (n_channels, n_times)\n",
    "\n",
    "    # Calculate the mean absolute value (over the time axis)\n",
    "    mean_abs_values = np.mean(np.abs(data), axis=1) # shape: (n_channels,)\n",
    "    \n",
    "    # Display the mean absolute value for each channel\n",
    "    for ch_name, mean_val in zip(raw_cropped.ch_names, mean_abs_values):\n",
    "        # print(f\"{ch_name} : mean absolute value = {mean_val:.3f} µV\")\n",
    "        print(f\"{ch_name} : mean absolute value = {mean_val:.8f} µV\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Stimuli not found in the annotations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce226326",
   "metadata": {},
   "source": [
    "# 9. Seperation in epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81248cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = -0.2  # 200 ms before the event\n",
    "tmax = 0.8   # 800 ms after the event\n",
    "epochs = mne.Epochs(raw, events, event_id=event_id,\n",
    "                    tmin=tmin, tmax=tmax, baseline=(None, 0),\n",
    "                    preload=True)\n",
    "epochs.plot_drop_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0937dfd2",
   "metadata": {},
   "source": [
    "# 10. Define an automatic reject of the artifacts (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs.plot_drop_log()\n",
    "# epochs.drop_bad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0092fb",
   "metadata": {},
   "source": [
    "# 11. Averaging (ERP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked = epochs.average()\n",
    "# evoked.plot(title=\"ERP (moyenne des epochs)\")\n",
    "evoked.plot() # evoked does not accept any \"title\"\n",
    "\n",
    "# To add a title to the graph\n",
    "# fig = evoked.plot_image(picks='eeg')\n",
    "# fig.suptitle(\"ERP (moyenne des epochs)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70678627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a title to the graph\n",
    "fig = evoked.plot_image(picks='eeg')\n",
    "fig.suptitle(\"ERP (moyenne des epochs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652beabb",
   "metadata": {},
   "source": [
    "# 12. Topography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evoked.plot_topomap(times=[0.1, 0.2, 0.3], ch_type='eeg', title=\"Topomap à 100/200/300 ms\")\n",
    "evoked.plot_topomap(times=[0.1, 0.2, 0.3], ch_type='eeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891ccb0",
   "metadata": {},
   "source": [
    "*** Show the Data ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12cf643",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
